---
title: "Create Hermes Publication Plots"
author: "Jürgen Hermes"
date: "2023-03-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) # Clear environment

source('./Entropy_Functions.R')
source('./Additional_Functions.R')
### This is a function which goes through texts and deletes uncommon letters
### The default threshold is .01%
library(Hmisc)

delete_uncommon_chars <- function (s, thresh = .0001) {
  
  char.table <- table(unlist(strsplit(s, "")))
  char.rat.table <- char.table / sum(char.table)
  s.del <- s
  
  for (c in names(char.rat.table)) {
    if (char.rat.table[c] <= thresh) {
      print(c)
      s.del <- str_remove_all(s.del, c)
    }
  }
  
  return (s.del)
}
```

## Purpose of this programming code

This is the (now slightly improved) code I used for my publication "Polygraphia III: The cipher that pretends to be an artificial language", which I presented at the [International Voynich Conference 2022](https://www.um.edu.mt/event/voynich2022) and which is published here: https://ceur-ws.org/Vol-3313/

## Examined Texts

Specifically, various characteristics of different texts are examined:
- The text of the Voynich Manuscript
- Early modern texts in different languages
- Texts encoded with the Polygraphia III method



```{r cars}
# Generate Hermes publication corpus
pathToFiles <- "Historical_texts/hermes_pub/" 
pathToCleanedFiles <- "Historical_texts/hermes_pub_clean/"
pathToCiphers <- "Historical_ciphers/generated/"
files <- list.files(pathToFiles)
for(file in files){
  aix <- scan(paste(pathToFiles, file, sep=''), what="character", sep="\n", comment.char = "#")
  #print(aix)
  aix.clean <- gsub('[^\x20-\x7E]', '', aix)
  aix.clean <- tolower(aix.clean)
  aix.clean <- gsub('\\[[0-9]*[a-z]\\]', '', aix.clean)
  aix.clean <- gsub('[01«»\\(\\)\\?:;\\.,\\!\\*]', '', aix.clean)
  aix.clean <- gsub('\\-', '', aix.clean)
  aix.clean <- gsub(' {2,}', ' ', aix.clean)
  aix.clean <- delete_uncommon_chars(aix.clean)
  
  aix.clean <- concatenate(aix.clean, collapse="")
 
  write(aix.clean, file = paste(pathToCleanedFiles, file, sep=''), sep = "")
}

voy.full.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Full Voynich Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
voy.a.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Voynich A Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
voy.b.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Voynich B Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
hist.caesar.text <- scan(paste(pathToCleanedFiles,"caesar_de_bello_gallico.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.dante.text <- scan(paste(pathToCleanedFiles,"dante_dc_inferno.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.dee.text <- scan(paste(pathToCleanedFiles,"Dee_elements.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.descartes.text <- scan(paste(pathToCleanedFiles,"descartes_meditationes.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.melanchthon.text <- scan(paste(pathToCleanedFiles,"melanchthon_confession.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.paracelsus.text <- scan(paste(pathToCleanedFiles,"paracelsus_paragranum.txt",sep=''), what="character", sep="\n", comment.char = "#")
pIII.10columns <- scan(paste(pathToCiphers,"PIII_10_columns",sep=''), what="character", sep="\n", comment.char = "#")
pIII.24columns <- scan(paste(pathToCiphers,"PIII_24_columns",sep=''), what="character", sep="\n", comment.char = "#")
pIII.allcolumns <- scan(paste(pathToCiphers,"PIII_all_columns",sep=''), what="character", sep="\n", comment.char = "#")


# Create a dataframe of overall statistics about the files

investigated.texts <- c(voy.full.max.simp.text,voy.a.max.simp.text,voy.b.max.simp.text,
                    hist.caesar.text,hist.dante.text,hist.dee.text,hist.descartes.text,hist.melanchthon.text,hist.paracelsus.text, 
                    pIII.10columns,pIII.24columns,pIII.allcolumns)

langs <-c('Full Voynich Maximal Simplified Text', 'Voynich A Maximal Simplified Text', 'Voynich A Maximal Simplified Text', 
          'Caesar Bello Gallico', 'Dante Inferno', 'Dee Elements', 'Descartes Meditationes', 'Melanchthon Confession', 'Paracelsus Panagranum', 
          'Polygraphia III small set', 'Polygraphia III medium set', 'Polygraphia III full set')

codes <- c('voy.f.s.text', 'voy.a.s.text', 'voy.b.s.text', 
           'caesar', 'dante', 'dee', 'descart', 'melanch', 'paracel', 
           'p3.10c', 'p3.24c', 'p3.131c')

fams <- c('Voynich','Voynich','Voynich',
          'Latin', 'Italian', 'English', 'Latin', 'German', 'German', 
          'Polygraphian','Polygraphian','Polygraphian')

scripts <- c('Voynich','Voynich','Voynich','Latin','Latin','Latin','Latin','Latin','Latin','Cipher','Cipher','Cipher')

```

## Examined Features

The features under investigation are: 
- The word length distribution of types and tokens
- Different entropy values
- The number of adjacency word repetitions and minimal pairs.

```{r pressure, echo=FALSE}
completeStats <- data.frame()
length <- c(1:20)
counts <- rep(0, 20)

repetitionStats <- data.frame()
entropyStats <- data.frame()
wordlengthStats <- data.frame()
word_length_tokens_all <- data.frame(word_length = integer(), frequency = integer())


for (v in investigated.texts) {
  
  doc <- v
  
  # Word length distributions 
  word_length_tokens <- tokenLengthDistibutions(doc)
  
  word_length_types <- typeLengthDistibutions(doc)
  print(word_length_tokens)
  print(word_length_types)
  
  # Calculate the mean and standard deviation for types
  mean_types <- sum(as.numeric(names(word_length_types)) * as.numeric(word_length_types)) / sum(as.numeric(word_length_types))
  sd_types <- sqrt((sum(as.numeric(names(word_length_types))-mean_types)^2 / sum(as.numeric(word_length_types))))
    
  # Calculate the mean and standard deviation for tokens
  mean_tokens <-  sum(as.numeric(names(word_length_tokens)) * as.numeric(word_length_tokens)) / sum(as.numeric(word_length_tokens))
  sd_tokens <- sqrt((sum(as.numeric(names(word_length_tokens))-mean_tokens)^2 / sum(as.numeric(word_length_tokens))))
  
  sqrt(sum(noten_haeufigkeit * (noten - gew_mittelwert)^2) / sum(noten_haeufigkeit))
  
  print(names(word_length_tokens))
  print(word_length_tokens)
  product <- as.numeric(names(word_length_tokens)) * as.numeric(word_length_tokens)
  print(product)
  lengthStats <- data.frame(mean_types, sd_types, mean_tokens, sd_tokens)
  wordlengthStats <-rbind(wordlengthStats, lengthStats)
  
  #Entropy values
  entropies <- charEntropies(v)
  entropyStats <- rbind(entropyStats,entropies)
  
  
  #Adjazent repetitions / minimal pairs
  minimalPairs <- list.rep.seqs(doc, dist=1, min.seq = 2, min.word = 4)
  minimalTriplets <- list.rep.seqs(doc, dist=1, min.seq = 3, min.word = 4)
  doubledWords <- list.rep.seqs(doc, dist=0, min.seq = 2, min.word = 4)
  tripledWords <- list.rep.seqs(doc, dist=0, min.seq = 3, min.word = 4)
  
  
  repcounts <- data.frame(doubledWords, tripledWords, minimalPairs, minimalTriplets) 
  repetitionStats <- rbind(repetitionStats,repcounts)

  #print(repcounts)
  #doc.df <- multi_stats(doc)

  #df <- rbind(df, doc.df)

}

# Plot the word length distributions for tokens for all texts
#library(ggplot2)
#ggplot(word_length_tokens_all, aes(x = word_length, y = frequency)) +
#  geom_line() +
#  labs(title = "Word Length Distributions for Tokens", x = "Word Length", y = "Frequency") +
#  theme_bw()


# Save the file
df <-data.frame(langs, codes, fams, scripts,wordlengthStats,entropyStats,repetitionStats)
write.csv(df, file='Historical_ciphers/hermes_stats.csv', row.names=FALSE)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
