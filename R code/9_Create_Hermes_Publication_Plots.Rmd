---
title: "Create Hermes Publication Plots"
author: "Jürgen Hermes"
date: "2023-03-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls()) # Clear environment

source('./Entropy_Functions.R')
source('./Additional_Functions.R')
### This is a function which goes through texts and deletes uncommon letters
### The default threshold is .01%
library(Hmisc)

delete_uncommon_chars <- function (s, thresh = .0001) {
  
  char.table <- table(unlist(strsplit(s, "")))
  char.rat.table <- char.table / sum(char.table)
  s.del <- s
  
  for (c in names(char.rat.table)) {
    if (char.rat.table[c] <= thresh) {
      print(c)
      s.del <- str_remove_all(s.del, c)
    }
  }
  
  return (s.del)
}
```

## Purpose of this programming code

This is the (now slightly improved) code I used for my publication "Polygraphia III: The cipher that pretends to be an artificial language", which I presented at the [International Voynich Conference 2022](https://www.um.edu.mt/event/voynich2022) and which is published here: https://ceur-ws.org/Vol-3313/

## Examined Texts

Specifically, various characteristics of different texts are examined:
- The text of the Voynich Manuscript
- Early modern texts in different languages
- Texts encoded with the Polygraphia III method


## Examined Features

The features under investigation are: 
- The word length distribution of types and tokens
- Different entropy values
- The number of adjacency word repetitions and minimal pairs.


```{r cars}
# Generate Hermes publication corpus
pathToFiles <- "Historical_texts/hermes_pub/" 
pathToCleanedFiles <- "Historical_texts/hermes_pub_clean/"
pathToCiphers <- "Historical_ciphers/generated/"
files <- list.files(pathToFiles)
for(file in files){
  aix <- scan(paste(pathToFiles, file, sep=''), what="character", sep="\n", comment.char = "#")
  #print(aix)
  aix.clean <- gsub('[^\x20-\x7E]', '', aix)
  aix.clean <- tolower(aix.clean)
  aix.clean <- gsub('\\[[0-9]*[a-z]\\]', '', aix.clean)
  aix.clean <- gsub('[01«»\\(\\)\\?:;\\.,\\!\\*]', '', aix.clean)
  aix.clean <- gsub('\\-', '', aix.clean)
  aix.clean <- gsub(' {2,}', ' ', aix.clean)
  aix.clean <- delete_uncommon_chars(aix.clean)
  
  aix.clean <- concatenate(aix.clean, collapse="")
 
  write(aix.clean, file = paste(pathToCleanedFiles, file, sep=''), sep = "")
}

voy.full.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Full Voynich Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
voy.a.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Voynich A Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
voy.b.max.simp.text <- scan('Voynich_texts/Maximal Simplified/Voynich B Maximal Simplified Text', what="character", sep="\n", comment.char = "#")
hist.caesar.text <- scan(paste(pathToCleanedFiles,"caesar_de_bello_gallico.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.dante.text <- scan(paste(pathToCleanedFiles,"dante_dc_inferno.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.dee.text <- scan(paste(pathToCleanedFiles,"Dee_elements.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.descartes.text <- scan(paste(pathToCleanedFiles,"descartes_meditationes.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.melanchthon.text <- scan(paste(pathToCleanedFiles,"melanchthon_confession.txt",sep=''), what="character", sep="\n", comment.char = "#")
hist.paracelsus.text <- scan(paste(pathToCleanedFiles,"paracelsus_paragranum.txt",sep=''), what="character", sep="\n", comment.char = "#")
pIII.10columns <- scan(paste(pathToCiphers,"PIII_10_columns",sep=''), what="character", sep="\n", comment.char = "#")
pIII.24columns <- scan(paste(pathToCiphers,"PIII_24_columns",sep=''), what="character", sep="\n", comment.char = "#")
pIII.allcolumns <- scan(paste(pathToCiphers,"PIII_all_columns",sep=''), what="character", sep="\n", comment.char = "#")


# Create a dataframe of overall statistics about the files

investigated.texts <- c(voy.full.max.simp.text,voy.a.max.simp.text,voy.b.max.simp.text,
                    hist.caesar.text,hist.dante.text,hist.dee.text,hist.descartes.text,hist.melanchthon.text,hist.paracelsus.text, 
                    pIII.10columns,pIII.24columns,pIII.allcolumns)

langs <-c('Full Voynich Maximal Simplified Text', 'Voynich A Maximal Simplified Text', 'Voynich A Maximal Simplified Text', 
          'Caesar Bello Gallico', 'Dante Inferno', 'Dee Elements', 'Descartes Meditationes', 'Melanchthon Confession', 'Paracelsus Panagranum', 
          'Polygraphia III small set', 'Polygraphia III medium set', 'Polygraphia III full set')

codes <- c('voy.f.s.text', 'voy.a.s.text', 'voy.b.s.text', 
           'caesar', 'dante', 'dee', 'descart', 'melanch', 'paracel', 
           'p3.10c', 'p3.24c', 'p3.131c')

fams <- c('Voynich','Voynich','Voynich',
          'Latin', 'Italian', 'English', 'Latin', 'German', 'German', 
          'Polygraphian','Polygraphian','Polygraphian')

scripts <- c('Voynich','Voynich','Voynich','Latin','Latin','Latin','Latin','Latin','Latin','Cipher','Cipher','Cipher')

plotcolors <-c('green','palegreen','lawngreen',    'grey90','gray80','grey70','grey60','grey50','grey40',    'red','tomato','darkorange')

print
completeStats <- data.frame()
length <- c(1:20)
counts <- rep(0, 20)

repetitionStats <- data.frame()
entropyStats <- data.frame()
wordlengthStats <- data.frame()
word_length_tokens_all <- data.frame(word_length = integer(), frequency = integer())


word_length_types_all <- table(type_length = c(1:36), frequency = rep(0,36))

for (v in investigated.texts) {
  
  doc <- v
  
  # Word length distributions 
  word_length_tokens <- tokenLengthDistibutions(doc)
  word_length_tokens_df <- data.frame(word_length = as.numeric(names(word_length_tokens)), frequency = as.numeric(word_length_tokens))
  
  # Append the data frame to the all data frame
  word_length_tokens_all <- merge(word_length_tokens_all, word_length_tokens_df, all=TRUE, by.x = "word_length", by.y="word_length")
  
  word_length_types <- typeLengthDistibutions(doc)
  word_length_types_all <- merge(word_length_types_all, word_length_types, all = TRUE)
  
  #print(word_length_tokens)
  #print(word_length_types)
  
  # Calculate the mean and standard deviation for types
  mean_types <- sum(as.numeric(names(word_length_types)) * as.numeric(word_length_types)) / sum(as.numeric(word_length_types))
  sd_types <- sqrt((sum(as.numeric(names(word_length_types))-mean_types)^2 / sum(as.numeric(word_length_types))))
  word_length_types_all <- cbind()
    
  # Calculate the mean and standard deviation for tokens
  mean_tokens <-  sum(as.numeric(names(word_length_tokens)) * as.numeric(word_length_tokens)) / sum(as.numeric(word_length_tokens))
  sd_tokens <- sqrt((sum(as.numeric(names(word_length_tokens))-mean_tokens)^2 / sum(as.numeric(word_length_tokens))))
  
  print(names(word_length_tokens))
  print(word_length_tokens)
  
  lengthStats <- data.frame(mean_types, sd_types, mean_tokens, sd_tokens)
  wordlengthStats <-rbind(wordlengthStats, lengthStats)
  
  #Entropy values
  entropies <- charEntropies(v)
  entropyStats <- rbind(entropyStats,entropies)
  
  
  #Adjazent repetitions / minimal pairs
  minimalPairs <- list.rep.seqs(doc, dist=1, min.seq = 2, min.word = 4)
  minimalTriplets <- list.rep.seqs(doc, dist=1, min.seq = 3, min.word = 4)
  doubledWords <- list.rep.seqs(doc, dist=0, min.seq = 2, min.word = 4)
  tripledWords <- list.rep.seqs(doc, dist=0, min.seq = 3, min.word = 4)
  
  
  repcounts <- data.frame(doubledWords, tripledWords, minimalPairs, minimalTriplets) 
  repetitionStats <- rbind(repetitionStats,repcounts)

}



word_length_tokens_all <-  word_length_tokens_all[,-2]
word_length_tokens_all <-t(word_length_tokens_all)
colnames(word_length_tokens_all) <- (word_length_tokens_all[1,])
word_length_tokens_all <- word_length_tokens_all[-1,]
rownames(word_length_tokens_all) <- codes
word_length_tokens_all[is.na(word_length_tokens_all)]<-0
# TODO: Relative values, type plots
#str(word_length_tokens_all)

plot(x=as.numeric(colnames(word_length_tokens_all)), y=word_length_tokens_all[1,], type = "l", col = "white")

for (i in seq(1, 12)) {
  lines(x=as.numeric(colnames(word_length_tokens_all)), y=word_length_tokens_all[i,], type = "l", col = plotcolors[i])
}

df <-data.frame(langs, codes, fams, scripts,wordlengthStats,entropyStats,repetitionStats)

write.csv(df, file='Historical_ciphers/hermes_stats.csv', row.names=FALSE)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
